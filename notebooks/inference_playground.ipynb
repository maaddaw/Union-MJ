{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Uuviq3qQkUFy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "798c3997-0cf4-4802-efa9-1197ed254803"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'encoder4editing'...\n",
            "remote: Enumerating objects: 172, done.\u001b[K\n",
            "remote: Counting objects: 100% (78/78), done.\u001b[K\n",
            "remote: Compressing objects: 100% (36/36), done.\u001b[K\n",
            "remote: Total 172 (delta 49), reused 42 (delta 42), pack-reused 94 (from 1)\u001b[K\n",
            "Receiving objects: 100% (172/172), 33.43 MiB | 20.35 MiB/s, done.\n",
            "Resolving deltas: 100% (59/59), done.\n",
            "--2025-06-27 18:38:10--  https://github.com/ninja-build/ninja/releases/download/v1.8.2/ninja-linux.zip\n",
            "Resolving github.com (github.com)... 140.82.116.4\n",
            "Connecting to github.com (github.com)|140.82.116.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://objects.githubusercontent.com/github-production-release-asset-2e65be/1335132/d2f252e2-9801-11e7-9fbf-bc7b4e4b5c83?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=releaseassetproduction%2F20250627%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20250627T183810Z&X-Amz-Expires=1800&X-Amz-Signature=6c20e78df7ef1fa890f403f243585e3fbb50cc29c93abb69b43ff0ca6dffc8f6&X-Amz-SignedHeaders=host&response-content-disposition=attachment%3B%20filename%3Dninja-linux.zip&response-content-type=application%2Foctet-stream [following]\n",
            "--2025-06-27 18:38:10--  https://objects.githubusercontent.com/github-production-release-asset-2e65be/1335132/d2f252e2-9801-11e7-9fbf-bc7b4e4b5c83?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=releaseassetproduction%2F20250627%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20250627T183810Z&X-Amz-Expires=1800&X-Amz-Signature=6c20e78df7ef1fa890f403f243585e3fbb50cc29c93abb69b43ff0ca6dffc8f6&X-Amz-SignedHeaders=host&response-content-disposition=attachment%3B%20filename%3Dninja-linux.zip&response-content-type=application%2Foctet-stream\n",
            "Resolving objects.githubusercontent.com (objects.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to objects.githubusercontent.com (objects.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 77854 (76K) [application/octet-stream]\n",
            "Saving to: ‘ninja-linux.zip’\n",
            "\n",
            "ninja-linux.zip     100%[===================>]  76.03K  --.-KB/s    in 0.01s   \n",
            "\n",
            "2025-06-27 18:38:11 (6.18 MB/s) - ‘ninja-linux.zip’ saved [77854/77854]\n",
            "\n",
            "Archive:  ninja-linux.zip\n",
            "  inflating: /usr/local/bin/ninja    \n",
            "update-alternatives: using /usr/local/bin/ninja to provide /usr/bin/ninja (ninja) in auto mode\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/utils/cpp_extension.py:2059: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. \n",
            "If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torch/utils/cpp_extension.py:2059: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. \n",
            "If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "#@title Setup Repository\n",
        "import os\n",
        "os.chdir('/content')\n",
        "CODE_DIR = 'encoder4editing'\n",
        "\n",
        "!git clone https://github.com/omertov/encoder4editing.git $CODE_DIR\n",
        "!wget https://github.com/ninja-build/ninja/releases/download/v1.8.2/ninja-linux.zip\n",
        "!sudo unzip ninja-linux.zip -d /usr/local/bin/\n",
        "!sudo update-alternatives --install /usr/bin/ninja ninja /usr/local/bin/ninja 1 --force\n",
        "os.chdir(f'./{CODE_DIR}')\n",
        "\n",
        "from argparse import Namespace\n",
        "import time\n",
        "import os\n",
        "import sys\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import torch\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "sys.path.append(\".\")\n",
        "sys.path.append(\"..\")\n",
        "\n",
        "from utils.common import tensor2im\n",
        "from models.psp import pSp  # we use the pSp framework to load the e4e encoder.\n",
        "\n",
        "%load_ext autoreload\n",
        "%autoreload 2"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Setup files downloader\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "download_with_pydrive = True #@param {type:\"boolean\"}\n",
        "\n",
        "class Downloader(object):\n",
        "    def __init__(self, use_pydrive):\n",
        "        self.use_pydrive = use_pydrive\n",
        "        current_directory = os.getcwd()\n",
        "        self.save_dir = os.path.join(os.path.dirname(current_directory), CODE_DIR, \"pretrained_models\")\n",
        "        os.makedirs(self.save_dir, exist_ok=True)\n",
        "        if self.use_pydrive:\n",
        "            self.authenticate()\n",
        "\n",
        "    def authenticate(self):\n",
        "        auth.authenticate_user()\n",
        "        gauth = GoogleAuth()\n",
        "        gauth.credentials = GoogleCredentials.get_application_default()\n",
        "        self.drive = GoogleDrive(gauth)\n",
        "\n",
        "    def download_file(self, file_id, file_name):\n",
        "        file_dst = f'{self.save_dir}/{file_name}'\n",
        "        if os.path.exists(file_dst):\n",
        "            print(f'{file_name} already exists!')\n",
        "            return\n",
        "        if self.use_pydrive:\n",
        "            downloaded = self.drive.CreateFile({'id':file_id})\n",
        "            downloaded.FetchMetadata(fetch_all=True)\n",
        "            downloaded.GetContentFile(file_dst)\n",
        "        else:\n",
        "            !gdown --id $file_id -O $file_dst\n",
        "\n",
        "downloader = Downloader(download_with_pydrive)"
      ],
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "g_pFcybblp-H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HRjtz6uLkTJs"
      },
      "source": [
        "## Step 1: Select Experiment Type\n",
        "Select which experiment you wish to perform inference on:\n",
        "1. ffhq_encode\n",
        "2. cars_encode\n",
        "3. horse_encode\n",
        "4. church_encode"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XESWAO65kTJt"
      },
      "outputs": [],
      "source": [
        "experiment_type = 'ffhq_encode' #@param ['ffhq_encode', 'cars_encode', 'horse_encode', 'church_encode']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4etDz82xkTJz"
      },
      "source": [
        "## Step 2: Download Pretrained Models\n",
        "As part of this repository, we provide pretrained models for each of the above experiments. We'll download the model for the selected experiments and save it to the folder `pretrained_models`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KSnjlBZOkTJ0"
      },
      "outputs": [],
      "source": [
        "#@title Download\n",
        "MODEL_PATHS = {\n",
        "    \"ffhq_encode\": {\"id\": \"1cUv_reLE6k3604or78EranS7XzuVMWeO\", \"name\": \"e4e_ffhq_encode.pt\"},\n",
        "    \"cars_encode\": {\"id\": \"17faPqBce2m1AQeLCLHUVXaDfxMRU2QcV\", \"name\": \"e4e_cars_encode.pt\"},\n",
        "    \"horse_encode\": {\"id\": \"1TkLLnuX86B_BMo2ocYD0kX9kWh53rUVX\", \"name\": \"e4e_horse_encode.pt\"},\n",
        "    \"church_encode\": {\"id\": \"1-L0ZdnQLwtdy6-A_Ccgq5uNJGTqE7qBa\", \"name\": \"e4e_church_encode.pt\"}\n",
        "}\n",
        "\n",
        "path = MODEL_PATHS[experiment_type]\n",
        "downloader.download_file(file_id=path[\"id\"], file_name=path[\"name\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Tozsg81kTKA"
      },
      "source": [
        "## Step 3: Define Inference Parameters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XIhyc7RqkTKB"
      },
      "source": [
        "Below we have a dictionary defining parameters such as the path to the pretrained model to use and the path to the image to perform inference on.  \n",
        "While we provide default values to run this script, feel free to change as needed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2kE5y1-skTKC"
      },
      "outputs": [],
      "source": [
        "EXPERIMENT_DATA_ARGS = {\n",
        "    \"ffhq_encode\": {\n",
        "        \"model_path\": \"pretrained_models/e4e_ffhq_encode.pt\",\n",
        "        \"image_path\": \"notebooks/images/input_img.jpg\"\n",
        "    },\n",
        "    \"cars_encode\": {\n",
        "        \"model_path\": \"pretrained_models/e4e_cars_encode.pt\",\n",
        "        \"image_path\": \"notebooks/images/car_img.jpg\"\n",
        "    },\n",
        "    \"horse_encode\": {\n",
        "        \"model_path\": \"pretrained_models/e4e_horse_encode.pt\",\n",
        "        \"image_path\": \"notebooks/images/horse_img.jpg\"\n",
        "    },\n",
        "    \"church_encode\": {\n",
        "        \"model_path\": \"pretrained_models/e4e_church_encode.pt\",\n",
        "        \"image_path\": \"notebooks/images/church_img.jpg\"\n",
        "    }\n",
        "\n",
        "}\n",
        "# Setup required image transformations\n",
        "EXPERIMENT_ARGS = EXPERIMENT_DATA_ARGS[experiment_type]\n",
        "if experiment_type == 'cars_encode':\n",
        "    EXPERIMENT_ARGS['transform'] = transforms.Compose([\n",
        "            transforms.Resize((192, 256)),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])])\n",
        "    resize_dims = (256, 192)\n",
        "else:\n",
        "    EXPERIMENT_ARGS['transform'] = transforms.Compose([\n",
        "        transforms.Resize((256, 256)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])])\n",
        "    resize_dims = (256, 256)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TAWrUehTkTKJ"
      },
      "source": [
        "## Step 4: Load Pretrained Model\n",
        "We assume that you have downloaded all relevant models and placed them in the directory defined by the above dictionary."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1t-AOhP1kTKJ"
      },
      "outputs": [],
      "source": [
        "model_path = EXPERIMENT_ARGS['model_path']\n",
        "ckpt = torch.load(model_path, map_location='cpu')\n",
        "opts = ckpt['opts']\n",
        "# pprint.pprint(opts)  # Display full options used\n",
        "# update the training options\n",
        "opts['checkpoint_path'] = model_path\n",
        "opts= Namespace(**opts)\n",
        "net = pSp(opts)\n",
        "net.eval()\n",
        "net.cuda()\n",
        "print('Model successfully loaded!')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4weLFoPbkTKZ"
      },
      "source": [
        "## Step 5: Visualize Input"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r2H9zFLJkTKa"
      },
      "outputs": [],
      "source": [
        "image_path = EXPERIMENT_DATA_ARGS[experiment_type][\"image_path\"]\n",
        "original_image = Image.open(image_path)\n",
        "original_image = original_image.convert(\"RGB\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o6oqf8JwzK0K"
      },
      "source": [
        "### Image Alignment\n",
        "For the FFHQ StyleGAN inversion, we first align the image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y244_ejy9Drx"
      },
      "outputs": [],
      "source": [
        "if experiment_type == \"ffhq_encode\" and 'shape_predictor_68_face_landmarks.dat' not in os.listdir():\n",
        "    !wget http://dlib.net/files/shape_predictor_68_face_landmarks.dat.bz2\n",
        "    !bzip2 -dk shape_predictor_68_face_landmarks.dat.bz2\n",
        "\n",
        "def run_alignment(image_path):\n",
        "  import dlib\n",
        "  from utils.alignment import align_face\n",
        "  predictor = dlib.shape_predictor(\"shape_predictor_68_face_landmarks.dat\")\n",
        "  aligned_image = align_face(filepath=image_path, predictor=predictor)\n",
        "  print(\"Aligned image has shape: {}\".format(aligned_image.size))\n",
        "  return aligned_image\n",
        "\n",
        "if experiment_type == \"ffhq_encode\":\n",
        "  input_image = run_alignment(image_path)\n",
        "else:\n",
        "  input_image = original_image\n",
        "\n",
        "input_image.resize(resize_dims)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D0BmXzu1kTKg"
      },
      "source": [
        "## Step 6: Perform Inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T3h3E7VLkTKg"
      },
      "outputs": [],
      "source": [
        "img_transforms = EXPERIMENT_ARGS['transform']\n",
        "transformed_image = img_transforms(input_image)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v5POMJ5YkTKl"
      },
      "outputs": [],
      "source": [
        "def display_alongside_source_image(result_image, source_image):\n",
        "    res = np.concatenate([np.array(source_image.resize(resize_dims)),\n",
        "                          np.array(result_image.resize(resize_dims))], axis=1)\n",
        "    return Image.fromarray(res)\n",
        "\n",
        "def run_on_batch(inputs, net):\n",
        "    images, latents = net(inputs.to(\"cuda\").float(), randomize_noise=False, return_latents=True)\n",
        "    if experiment_type == 'cars_encode':\n",
        "        images = images[:, :, 32:224, :]\n",
        "    return images, latents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ls5zb0fRkTKs"
      },
      "outputs": [],
      "source": [
        "with torch.no_grad():\n",
        "    tic = time.time()\n",
        "    images, latents = run_on_batch(transformed_image.unsqueeze(0), net)\n",
        "    result_image, latent = images[0], latents[0]\n",
        "    toc = time.time()\n",
        "    print('Inference took {:.4f} seconds.'.format(toc - tic))\n",
        "\n",
        "# Display inversion:\n",
        "display_alongside_source_image(tensor2im(result_image), input_image)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EH0wpgC3lp-b"
      },
      "source": [
        "## Edit"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nkabIzKelp-b"
      },
      "outputs": [],
      "source": [
        "from editings import latent_editor\n",
        "is_cars = experiment_type == 'cars_encode'\n",
        "editor = latent_editor.LatentEditor(net.decoder, is_cars)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vEebc46tlp-c"
      },
      "outputs": [],
      "source": [
        "# InterFaceGAN\n",
        "interfacegan_directions = {\n",
        "    'ffhq_encode': {\n",
        "        'age': 'editings/interfacegan_directions/age.pt',\n",
        "        'smile': 'editings/interfacegan_directions/smile.pt',\n",
        "        'pose': 'editings/interfacegan_directions/pose.pt'\n",
        "    }\n",
        "}\n",
        "available_interfacegan_directions = None\n",
        "if experiment_type in interfacegan_directions:  # List supported directions for the current experiment\n",
        "    available_interfacegan_directions = interfacegan_directions[experiment_type]\n",
        "    print(list(available_interfacegan_directions.keys()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qgx4vIw3lp-d"
      },
      "outputs": [],
      "source": [
        "# As an example, we currently released the age and smile directions for the FFHQ StyleGAN Generator.\n",
        "interfacegan_direction = torch.load(available_interfacegan_directions[\"age\"]).cuda()\n",
        "\n",
        "# For a single edit:\n",
        "result = editor.apply_interfacegan(latents, interfacegan_direction, factor=-3).resize(resize_dims)\n",
        "display_alongside_source_image(result, input_image)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sSsY-itKlp-d"
      },
      "outputs": [],
      "source": [
        "# Or, apply a range of editings\n",
        "editor.apply_interfacegan(latents, interfacegan_direction, factor_range=(-5, 5))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jdgOBMszlp-d"
      },
      "outputs": [],
      "source": [
        "# GANSpace\n",
        "# Here we provide the editings for the cars domain as displayed in the paper, as well as several examples for the facial domain,\n",
        "# taken from the official GANSpace repository.\n",
        "if experiment_type == 'ffhq_encode':\n",
        "    ganspace_pca = torch.load('editings/ganspace_pca/ffhq_pca.pt')\n",
        "    directions = {\n",
        "        'eye_openness':            (54,  7,  8,  20),\n",
        "        'smile':                   (46,  4,  5, -20),\n",
        "        'trimmed_beard':           (58,  7,  9,  20),\n",
        "        'white_hair':              (57,  7, 10, -24),\n",
        "        'lipstick':                (34, 10, 11,  20)\n",
        "    }\n",
        "elif experiment_type == 'cars_encode':\n",
        "    ganspace_pca = torch.load('editings/ganspace_pca/cars_pca.pt')\n",
        "    directions = {\n",
        "        \"Viewpoint I\": (0, 0, 5, 2),\n",
        "        \"Viewpoint II\": (0, 0, 5, -2),\n",
        "        \"Cube\": (16, 3, 6, 25),\n",
        "        \"Color\": (22, 9, 11, -8),\n",
        "        \"Grass\": (41, 9, 11, -18),\n",
        "    }\n",
        "print(f'Available Editings: {list(directions.keys())}')\n",
        "editor.apply_ganspace(latents, ganspace_pca, [directions[\"white_hair\"], directions[\"eye_openness\"], directions[\"smile\"]])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZrfrgXuYlp-e"
      },
      "outputs": [],
      "source": [
        "# SeFa\n",
        "# Note that each model behaves differently to the selected editing parameters.\n",
        "# We encourage the user to try out different configurations, using different indices, start/end_distance, etc.\n",
        "# In the paper, we used start and end distance of -15.0, +15.0 over the horses and churches domains.\n",
        "# See code at editings/sefa.py for further options.\n",
        "editor.apply_sefa(latents, indices=[2, 3, 4, 5], start_distance=0., end_distance=15.0, step=3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G9qKRItzlp-e"
      },
      "outputs": [],
      "source": [
        "# Note, that for Styleflow editings,\n",
        "# one need to save the output latent codes and load them over the official StyleFlow repository:\n",
        "# torch.save(latents, 'latents.pt')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "id": "CAG494yvqer0"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "inference_playground.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}